\documentclass[12pt,english]{article}
\usepackage[a4paper,bindingoffset=0.2in,%
            left=1in,right=1in,top=1in,bottom=1in,%
            footskip=.25in]{geometry}
\usepackage{blindtext}
\usepackage{titling}
\usepackage{amssymb}
\usepackage{listofitems,amsmath}
\usepackage{listings}
\usepackage{lettrine} 
\usepackage{tikz}  
\usepackage{color} 
 \usetikzlibrary{shapes, arrows, calc, arrows.meta, fit, positioning} % these are the parameters passed to the library to create the node graphs  
\tikzset{  
    -Latex,auto,node distance =0.6 cm and 1.3 cm, thick,% node distance is the distance between one node to other, where 1.5cm is the length of the edge between the nodes  
    state/.style ={ellipse, draw, minimum width = 0.9 cm}, % the minimum width is the width of the ellipse, which is the size of the shape of vertex in the node graph  
    point/.style = {circle, draw, inner sep=0.18cm, fill, node contents={}},  
    bidirected/.style={Latex-Latex,dashed}, % it is the edge having two directions  
    el/.style = {inner sep=2.5pt, align=right, sloped}  
}  
\setlength{\parskip}{12pt}
%================================
\begin{document}
\newgeometry{left=0.8in,right=0.8in,top=1in,bottom=1in}
\begin{center}
    \Large
    \textbf{Homework 2}\\
    \small
    \today\\
    \large
    Jos\'{e} Carlos Mu\~{n}oz
\end{center}
excersize 1)\\
we know that
\begin{align*}
x_1&=2 & x_2&= 3 & \frac{\partial L}{\partial o}&=5\\
     &     & o   &= x_1 * x_2 
\end{align*}
To find $\frac{\partial L}{\partial x_1}$ and $\frac{\partial L}{\partial x_2}$ we use the Chain rule which gives us $\frac{\partial L}{\partial o}\frac{\partial o}{\partial x_1}$ and $\frac{\partial L}{\partial o}\frac{\partial o}{\partial x_2}$ respectively. It can be derived that $\frac{\partial o}{\partial x_1}$ and $\frac{\partial o}{\partial x_2}$ are $x_2$ and $x_1$ respectively\\
Therefore we can solve for both\\
\begin{align*}
\frac{\partial L}{\partial x_1}&=\frac{\partial L}{\partial o}\frac{\partial o}{\partial x_1} & \frac{\partial L}{\partial x_2}&=\frac{\partial L}{\partial o}\frac{\partial o}{\partial x_2}\\
                                      &=5*x_2  &  &=5 *x_1\\
                                      &=5*3  &  &=5 *2\\
\frac{\partial L}{\partial x_1}&=15  &  \frac{\partial L}{\partial x_2}&=10\\
\end{align*}
excersize 2)\\
The Neural Network is as follows.
% Input layer neurons'number
\newcommand{\inputnum}{2}  
% Hidden layer neurons'number
\newcommand{\hiddennum}{2}   
% Output layer neurons'number
\newcommand{\outputnum}{1}  
\begin{center}
\begin{tikzpicture} 
% Input Layer
    \node[circle, 
        minimum size = 6mm,
        fill=orange!30] (Input-1) at (0,-1) {$x_1$};
    \node[circle, 
        minimum size = 6mm,
        fill=orange!30] (Input-2) at (0,-3) {$x_2$};
% Hidden Layer
\node[circle, 
        minimum size = 6mm,
        fill=teal!50,
        yshift=(\hiddennum-\inputnum)*5 mm
    ] (Hidden-1) at (2.5,-1) {$C$};
\node[circle, 
        minimum size = 6mm,
        fill=teal!50,
        yshift=(\hiddennum-\inputnum)*5 mm
    ] (Hidden-2) at (2.5,-3) {$D$};
% Output Layer
    \node[circle, 
        minimum size = 6mm,
        fill=purple!50,
        yshift=(\outputnum-\inputnum)*5 mm
    ] (Output-1) at (5,-1.5) {$E$}; 
% Connect neurons In-Hidden
\draw[->, shorten >=1pt] (Input-1) -- (Hidden-1)node[midway,above left] {$w_1$};
\draw[->, shorten >=1pt] (Input-1) -- (Hidden-2)node[midway,above left] {$w_2$};
\draw[->, shorten >=1pt] (Input-2) -- (Hidden-1)node[midway,below left] {$w_3$};
\draw[->, shorten >=1pt] (Input-2) -- (Hidden-2)node[midway,below left] {$w_4$};
% Connect neurons Hidden-Out
\draw[->, shorten >=1pt] (Hidden-1) -- (Output-1)node[midway,above] {$w_5$};
\draw[->, shorten >=1pt] (Hidden-2) -- (Output-1)node[midway,above] {$w_6$};
\end{tikzpicture}
\end{center}
We know that $w_1 = 0.1$, $w_2 = 0.5$, $w_3 = 0.4$, $w_4 = 0.3$, $w_5 = 0.2$, $w_6 = 0.6$. The Hidden Layer and Actrivation Layer,$y_h()$ and $y_o()$ respectively , both have the activation function of $y_n(z) = \frac{1}{1 + e^{-z}}$. The Loss function is $L = \frac{1}{2} (y - \hat{y})^{2}$, where y is the expected value and $\hat{y}$ is the actual value\\
Our starting point is $\begin{pmatrix} \begin{bmatrix} 0.82 \\ 0.23 \end{bmatrix}  & 0 \end{pmatrix}$.
The Weights for each nodes are as follows
\begin{align*} 
\vec{w_C} &= \begin{bmatrix} w_1 \\ w_3 \end{bmatrix} &
\vec{w_D} &= \begin{bmatrix} w_2 \\ w_4 \end{bmatrix} &
\vec{w_E} &= \begin{bmatrix} w_5 \\ w_6 \end{bmatrix}
\end{align*}
The Weights for the Hidden and Output layer as as follow
\begin{align*} 
W_h &= \begin{bmatrix} w_C & w_D \end{bmatrix} &
W_o &= \begin{bmatrix} w_E  \end{bmatrix} \\
&= \begin{bmatrix} w_1 & w_2 \\ w_3 & w_4 \end{bmatrix} &
&= \begin{bmatrix} w_5 \\ w_6 \end{bmatrix}
\end{align*}
The $z_C$, $z_D$ and $z_E$ can be written as followed
\begin{align*} 
z_C &= w_1 * x_1 +w3 * x_2 &
z_D &= w_2 * x_1 +w4 * x_2 &
z_E &= w_5 * y_C + w_6 * y_D \\
\end{align*}
For  solving forward propogation we do the following steps
\begin{align}
W^T_h * \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}  &= \begin{bmatrix} z_C \\ z_D \end{bmatrix} \\
\begin{bmatrix} y_n(z_C) \\ y_n(z_D) \end{bmatrix} &= \begin{bmatrix} y_C \\ y_D \end{bmatrix} \\
W^T_o * \begin{bmatrix} y_C \\ y_D \end{bmatrix} &=z_E\\
y_E &= y_n(z_E)
\end{align}
Plugging in the values we get this
\begin{align*}
\begin{bmatrix} 0.1 & 0.5 \\ 0.4 & 0.3 \end{bmatrix}^T * \begin{bmatrix} 0.82 \\ 0.23 \end{bmatrix}  &= \begin{bmatrix} 0.174 \\ 0.479 \end{bmatrix} \\
\begin{bmatrix} y_h(0.174) \\ y_h(0.479) \end{bmatrix} &= \begin{bmatrix} 0.5433906 \\ 0.6175177 \end{bmatrix} \\
\begin{bmatrix} 0.2 \\ 0.6 \end{bmatrix}^T * \begin{bmatrix} 0.5433906 \\ 0.6175177 \end{bmatrix} &=0.47918874\\
y_o(0.47918874) &= 0.617556289
\end{align*}
The ending value, $y_E$, is 0.617556289 and the other values are as follows\\
\begin{align*}
y_C &=0.5433906 & y_D&=0.6175177 \\
 z_C &=0.174 & y_D&=0.479 & z_E&=0.471988174
\end{align*}
For Back propagation, the outer layer weights will derived as
\begin{align*}
\frac{\partial L}{\partial w_5} &=\frac{\partial L}{\partial y_e} * \frac{\partial y_e}{\partial z_e} * \frac{\partial z_e}{\partial w_5} &
\frac{\partial L}{\partial w_6} &=\frac{\partial L}{\partial y_e} * \frac{\partial y_e}{\partial z_e} * \frac{\partial z_e}{\partial w_6} &
\delta_o &= \frac{\partial L}{\partial y_e} * \frac{\partial y_e}{\partial z_e}\\
\frac{\partial L}{\partial w_5} &= \delta_o * \frac{\partial z_e}{\partial w_5} &
\frac{\partial L}{\partial w_6} &= \delta_o * \frac{\partial z_e}{\partial w_6} &
\end{align*}
 For the weights that connect to Node C, we find the rate of change as
\begin{align*}
\frac{\partial L}{\partial w_1} &=\frac{\partial L}{\partial y_c} * \frac{\partial y_c}{\partial z_c} * \frac{\partial z_c}{\partial w_1} &
\frac{\partial L}{\partial w_3} &=\frac{\partial L}{\partial y_c} * \frac{\partial y_c}{\partial z_c} * \frac{\partial z_c}{\partial w_3} &
 \frac{\partial L}{\partial y_c}  &= \frac{\partial L}{\partial y_e} * \frac{\partial y_e}{\partial z_e} * \frac{\partial z_e}{\partial y_c}\\
& &&&   &= \delta_o * \frac{\partial z_e}{\partial y_c} 
\end{align*}
 For the weights that connect to Node D, we find the rate of change as
\begin{align*}
\frac{\partial L}{\partial w_2} &=\frac{\partial L}{\partial y_d} * \frac{\partial y_d}{\partial z_d} * \frac{\partial z_d}{\partial w_2} &
\frac{\partial L}{\partial w_4} &=\frac{\partial L}{\partial y_d} * \frac{\partial y_d}{\partial z_d} * \frac{\partial z_d}{\partial w_4} &
 \frac{\partial L}{\partial y_d}  &= \frac{\partial L}{\partial y_e} * \frac{\partial y_e}{\partial z_e} * \frac{\partial z_e}{\partial y_d} \\
& &&&  &= \delta_o * \frac{\partial z_e}{\partial y_d} \\
\end{align*}
Derivatives for L, y  are as follow
\begin{align*}
\frac{\partial L}{\partial y_e} &= y -y_e &
 \frac{\partial y_n}{\partial z_n}&= y_n(z_n) * (1 - y_n(z_n))
\end{align*}
Now lets solve for each weights\\
weights $w_5$ and $w_6$
\begin{align*}
\delta_o &=(0 - y_E) * (y_n(z_E) *(1 -y_n(z_E))\\
 &=0.09032574\\
\frac{\partial L}{\partial w_5} &=\delta_o* \frac{\partial z_e}{\partial w_5} &
\frac{\partial L}{\partial w_6} &=\delta_o * \frac{\partial z_e}{\partial w_6} \\
&=\delta_o * y_C &
&=\delta_o * y_D \\
&=0.09032574 * 0.5433906 &
&=0.09032574 * 0.6175177 \\
&=0.049082158&
&=0.055777743\\
\end{align*}
weights $w_1$ and $w_3$
\begin{align*}
 \frac{\partial L}{\partial y_C}  &= \delta_o * \frac{\partial z_e}{\partial y_C}\\
&= \delta_o * w_5\\
&= 0.09032574 * 0.2\\
&=0.018065148\\
\frac{\partial L}{\partial w_1} &=\frac{\partial L}{\partial y_C} * \frac{\partial y_C}{\partial z_C} * \frac{\partial z_C}{\partial w_1} &
\frac{\partial L}{\partial w_3} &=\frac{\partial L}{\partial y_C} * \frac{\partial y_C}{\partial z_C} * \frac{\partial z_C}{\partial w_3} \\
&=0.018065148 * (y_n(z_C) *(1 -y_n(z_C)) *0.82 &
&=0.018065148 * (y_n(z_C) *(1 -y_n(z_C)) * 0.23\\
&=0.018065148 * 0.248117255 * 0.82 &
&=0.018065148 * 0.248117255 * 0.23\\
&=0.00367546544 &
&=0.00103092323\\
\end{align*}
weights $w_2$ and $w_4$
\begin{align*}
 \frac{\partial L}{\partial y_D}  &= \delta_o * \frac{\partial z_e}{\partial y_D}\\
&= \delta_o * w_6\\
&= 0.09032574 * 0.6\\
&= 0.05419544\\
\frac{\partial L}{\partial w_2} &=\frac{\partial L}{\partial y_D} * \frac{\partial y_D}{\partial z_D} * \frac{\partial z_D}{\partial w_2} &
\frac{\partial L}{\partial w_4} &=\frac{\partial L}{\partial y_D} * \frac{\partial y_D}{\partial z_D} * \frac{\partial z_D}{\partial w_4} \\
&=0.05419544 * (y_n(z_D) *(1 -y_n(z_D)) * 0.82 &
&=0.05419544 * (y_n(z_D) *(1 -y_n(z_D)) * 0.23\\
&=0.05419544 * 0.23618959 * 0.82 &
&=0.05419544 * 0.23618959 * 0.23\\
&=0.01049632697 &
&=0.00294409171\\
\end{align*}
The Final $\frac{\partial L}{\partial\vec{w}}$ is as follows
\begin{align*}
\frac{\partial L}{\partial\vec{w}} &= \begin{bmatrix} 0.00367546544 \\0.01049632697 \\0.00103092323\\0.00294409171\\0.049082158\\0.055777743\end{bmatrix}
\end{align*}
The learning rate $\epsilon$ is 0.7 so the new weights are as followed
\begin{align*}
\vec{w^\prime} &=\vec{w} - \epsilon* \frac{\partial L}{\partial \vec{w}}\\
&=\begin{bmatrix} 0.1\\0.5\\0.4\\0.3\\0.2\\0.6\end{bmatrix}- 
0.7* \begin{bmatrix} 0.00367546544 \\0.01049632697 \\0.00103092323\\0.00294409171\\0.049082158\\0.055777743\end{bmatrix}\\
&= \begin{bmatrix} 0.09742717419 \\0.49265257112\\0.39927835373\\0.2979391358\\0.1656424894\\0.560955799\end{bmatrix}\\
\end{align*}
\end{document}