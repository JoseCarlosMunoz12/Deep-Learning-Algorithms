\documentclass[12pt,english]{article}
\usepackage[a4paper,bindingoffset=0.2in,%
            left=1in,right=1in,top=1in,bottom=1in,%
            footskip=.25in]{geometry}
\usepackage{blindtext}
\usepackage{titling}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{lettrine} 
\usepackage{tikz}  
\usepackage{color} 
\AtBeginEnvironment{align}{\setcounter{equation}{0}}
 \usetikzlibrary{shapes, arrows, calc, arrows.meta, fit, positioning} % these are the parameters passed to the library to create the node graphs  
\tikzset{  
    -Latex,auto,node distance =0.6 cm and 1.3 cm, thick,% node distance is the distance between one node to other, where 1.5cm is the length of the edge between the nodes  
    state/.style ={ellipse, draw, minimum width = 0.9 cm}, % the minimum width is the width of the ellipse, which is the size of the shape of vertex in the node graph  
    point/.style = {circle, draw, inner sep=0.18cm, fill, node contents={}},  
    bidirected/.style={Latex-Latex,dashed}, % it is the edge having two directions  
    el/.style = {inner sep=2.5pt, align=right, sloped}  
}  
\setlength{\parskip}{12pt}
%================================
\begin{document}
\newgeometry{left=0.8in,right=0.8in,top=1in,bottom=1in}
\begin{center}
    \Large
    \textbf{Homework 1}\\
    \small
    \today\\
    \large
    Jos\'{e} Carlos Mu\~{n}oz
\end{center}
excersize 1)\\
we know that
\begin{align*}
x_1&=2 & x_2&= 3 & \frac{\delta L}{\delta o}&=5\\
     &     & o   &= x_1 * x_2 
\end{align*}
To find $\frac{\delta L}{\delta x_1}$ and $\frac{\delta L}{\delta x_2}$ we use the Chain rule which gives us $\frac{\delta L}{\delta o}\frac{\delta o}{\delta x_1}$ and $\frac{\delta L}{\delta o}\frac{\delta o}{\delta x_2}$ respectively. It can be derived that $\frac{\delta o}{\delta x_1}$ and $\frac{\delta o}{\delta x_2}$ are $x_2$ and $x_1$ respectively\\
Therefore we can solve for both\\
\begin{align*}
\frac{\delta L}{\delta x_1}&=\frac{\delta L}{\delta o}\frac{\delta o}{\delta x_1} & \frac{\delta L}{\delta x_2}&=\frac{\delta L}{\delta o}\frac{\delta o}{\delta x_2}\\
                                      &=5*x_2  &  &=5 *x_1\\
                                      &=5*3  &  &=5 *2\\
\frac{\delta L}{\delta x_1}&=15  &  \frac{\delta L}{\delta x_2}&=10\\
\end{align*}
excersize 2)\\
The Neural Network is as follows.
% Input layer neurons'number
\newcommand{\inputnum}{2}  
% Hidden layer neurons'number
\newcommand{\hiddennum}{2}   
% Output layer neurons'number
\newcommand{\outputnum}{1}  
\begin{center}
\begin{tikzpicture} 
% Input Layer
    \node[circle, 
        minimum size = 6mm,
        fill=orange!30] (Input-1) at (0,-1) {$x_1$};
    \node[circle, 
        minimum size = 6mm,
        fill=orange!30] (Input-2) at (0,-3) {$x_2$};
% Hidden Layer
\node[circle, 
        minimum size = 6mm,
        fill=teal!50,
        yshift=(\hiddennum-\inputnum)*5 mm
    ] (Hidden-1) at (2.5,-1) {$C$};
\node[circle, 
        minimum size = 6mm,
        fill=teal!50,
        yshift=(\hiddennum-\inputnum)*5 mm
    ] (Hidden-2) at (2.5,-3) {$D$};
% Output Layer
    \node[circle, 
        minimum size = 6mm,
        fill=purple!50,
        yshift=(\outputnum-\inputnum)*5 mm
    ] (Output-1) at (5,-1.5) {$E$}; 
% Connect neurons In-Hidden
\draw[->, shorten >=1pt] (Input-1) -- (Hidden-1)node[midway,above left] {$w_1$};
\draw[->, shorten >=1pt] (Input-1) -- (Hidden-2)node[midway,above left] {$w_2$};
\draw[->, shorten >=1pt] (Input-2) -- (Hidden-1)node[midway,below left] {$w_3$};
\draw[->, shorten >=1pt] (Input-2) -- (Hidden-2)node[midway,below left] {$w_4$};
% Connect neurons Hidden-Out
\draw[->, shorten >=1pt] (Hidden-1) -- (Output-1)node[midway,above] {$w_5$};
\draw[->, shorten >=1pt] (Hidden-2) -- (Output-1)node[midway,above] {$w_6$};
\end{tikzpicture}
\end{center}
We know that $w_1 = 0.1$, $w_2 = 0.5$, $w_3 = 0.4$, $w_4 = 0.3$, $w_5 = 0.2$, $w_6 = 0.6$. The Hidden Layer and Actrivation Layer both have the activation function of $y_n(z) = \frac{1}{1 + e^{-z}}$. The Loss function is $L = \frac{1}{2} (y - \hat{y})^{2}$.\\
Our starting point is $\begin{pmatrix} \begin{bmatrix} 0.82 \\ 0.23 \end{bmatrix}  & 0 \end{pmatrix}$.
The Weights for each nodes are as follows
\begin{align*} 
\vec{w_C} &= \begin{bmatrix} w_1 \\ w_3 \end{bmatrix} &
\vec{w_D} &= \begin{bmatrix} w_2 \\ w_4 \end{bmatrix} &
\vec{w_E} &= \begin{bmatrix} w_5 \\ w_6 \end{bmatrix}
\end{align*}
The Weights for the Hidden and Output layer as as follow
\begin{align*} 
W_h &= \begin{bmatrix} w_C & w_D \end{bmatrix} &
W_o &= \begin{bmatrix} w_E  \end{bmatrix} \\
&= \begin{bmatrix} w_1 & w_2 \\ w_3 & w_4 \end{bmatrix} &
&= \begin{bmatrix} w_5 \\ w_6 \end{bmatrix}
\end{align*}
For  solving forward propogation we do the following steps
\begin{align}
W^T_h * \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}  &= \begin{bmatrix} z_C \\ z_D \end{bmatrix} \\
\begin{bmatrix} y_n(z_C) \\ y_n(z_D) \end{bmatrix} &= \begin{bmatrix} y_C \\ y_D \end{bmatrix} \\
W^T_o * \begin{bmatrix} y_C \\ y_D \end{bmatrix} &=z_E\\
y_E &= y_n(z_E)
\end{align}
Plugging in the values we get this
\begin{align}
\begin{bmatrix} 0.1 & 0.5 \\ 0.4 & 0.3 \end{bmatrix}^T * \begin{bmatrix} 0.82 \\ 0.23 \end{bmatrix}  &= \begin{bmatrix} 0.174 \\ 0.479 \end{bmatrix} \\
\begin{bmatrix} y_n(0.174) \\ y_n(0.479) \end{bmatrix} &= \begin{bmatrix} 0.5433906 \\ 0.6175177 \end{bmatrix} \\
\begin{bmatrix} 0.2 \\ 0.6 \end{bmatrix}^T * \begin{bmatrix} 0.5433906 \\ 0.6175177 \end{bmatrix} &=0.47918874\\
y_n(0.47918874) &= 0.617556289
\end{align}
The ending value, $y_E$, is 0.617556289\\
For backward propogation we  will be using these equations
\end{document}
