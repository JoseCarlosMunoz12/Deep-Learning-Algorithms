{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward NN with TF/KS\n",
    "Always start with importing Tensorflow(tf) and keras (ks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do machine learning TF/KS, you are likely to need to define, save, and restore a model.\n",
    "A model is (in abstract):\n",
    "<ol>\n",
    "    <li> A parameterized function that computes something on tensors that are assigned as values to function variables (a¬†forward pass) </li>\n",
    "    <li> The function can be updated (trained) when given examples and a loss function - cost of not matching target varaible value in examples</li>     \n",
    "    <li> To update function some variables can be updated using loss on previous training instances\n",
    "        <ol>\n",
    "            <li> some variables can be trainable others non-trainable</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li> Updates are the set of functions (one per network weight) that are computed taken loss tensors as input</li>\n",
    "</ol>\n",
    "Models are made of layers. \n",
    "<ol>\n",
    "<li>Layers are functions with a known mathematical structure that can be reused and have trainable variables. </li>\n",
    "<li>In TF, most high-level implementations of layers and models including Keras, are built on the same foundational class:¬†tf.Module</li>\n",
    "<li> tf. Module are named container for¬†tf.Variables, other¬†tf.Module and functions which apply to user input. In python terminology tf.module is an empty container class. It is sub-classed to create a true class.</li>\n",
    "<lI>Any class that is inherited from tf.module has internal state, and methods that use that state.</li>\n",
    "</ol>\n",
    "So models and layers are classes and objects in Python.\n",
    "\n",
    "## Layers and Models in TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(tf.Module):\n",
    "    def __init__(self,name=None):\n",
    "        super().__init__(name=name)\n",
    "        self._w_variable  = tf.Variable(1., shape=tf.TensorShape(None))\n",
    "        self._b_variable = tf.Variable(0.,shape=tf.TensorShape(None))\n",
    "        self.__first_run=tf.Variable(0, shape=[],trainable=False)\n",
    "       \n",
    "    def __call__(self, x, w = None, b = None):\n",
    "        if w!=None and self.__first_run==0:\n",
    "            if len(w.shape) == 1:\n",
    "                w = tf.reshape(X, [w.shape[0], 1])\n",
    "            self.__first_run.assign(1)\n",
    "            self._w_variable.assign(w)\n",
    "            if b != None:\n",
    "                self._b_variable.assign(b)\n",
    "        return tf.matmul(self._w_variable,x) + self._b_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in terms of Python Mymodule is a  module from which\n",
    "<ul>\n",
    "    <li> inherit initialiation if the name is given. But if it is not then this is our initialization here.</li>\n",
    "    <li> MyModule is callable class - i.e. it has method __call__ and can be called by name not only for instantiations but also for computations after object instantiation without method attribute.</li>\n",
    "</ul>\n",
    "Also I have not assigned shapes to variables which I will do on my first run, so that I can have any number of neurons at this instantiated by this module. In the cell below I have 2-dim vector as input and I have 3 neurons with weights (1,20, (2,3) and (3,4) respectively.  \n",
    "\n",
    "Now that I have module that does forward computation I can call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_module = MyModule()\n",
    "w_0 = tf.constant([[1.0,2.0],[2.0,3.0],[3.0,4.0]])\n",
    "b_0 = [[1.0],[2.0],[1.0]]\n",
    "x=[[1.0],[1.0]]\n",
    "p=my_module(x,w_0,b_0).numpy()\n",
    "print(\"forward computation:\",p)\n",
    "print(\"trainable variables:\", my_module.trainable_variables)\n",
    "print(\"all variables:\", my_module.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectively MyModule created a class that is a layer of linear neurons. Now I can create a model that consists of two linear layers such that output of one layer is fed into input of another layer. Hearby I connect two layers seququentially by feeding output of one layer into intput of another. If the second layer is all I need to do, then I created the model for output vairables  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self._layer_1 = MyModule()\n",
    "        self._layer_2 = MyModule()\n",
    "    \n",
    "    def __call__(self, x, w1 = None, b1 = None, w2 = None, b2 = None):\n",
    "        if w1==None:\n",
    "            y_1 = self._layer_1(x)\n",
    "            return self._layer_2(y_1)\n",
    "        else:\n",
    "            y_1 = self._layer_1(x,w1,b1)\n",
    "            return self._layer_2(y_1,w2,b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can instantiate my model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model=MyModel()\n",
    "w_1 = tf.constant([[1.0,2.0],[2.0,3.0],[3.0,4.0]])\n",
    "b_1 = [[1.0],[2.0],[1.0]]\n",
    "w_2 = tf.constant([[1.0,2.0,3.0],[3.0,4.0,5.0]])\n",
    "b_2 = [[1.0],[1.0]]\n",
    "x=[[1.0],[1.0]]\n",
    "y_hat=my_model(x,w_1,b_1,w_2,b_2).numpy()\n",
    "print(\"forward computation:\",y_hat)\n",
    "print(\"trainable variables:\", my_model.trainable_variables)\n",
    "print(\"all variables:\", my_model.variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers and Models in Keras\n",
    "So far we were doing it all in TF - NN 'assembler'. It is much easier to do it in Keras, a NN language written in TF, that allready has many necessary concepts prebuilt. The mayn concept in KS is the 'layer' that as its own class. A layer encapsulates both a state (the ‚Äùweights‚Äô‚Äô + ‚Äô‚Äôbias‚Äù) and a transformation of inputs to outputs (a \"call\", the layer's forward pass).  \n",
    "\n",
    "Example below defines linear layer with \n",
    "<ul>\n",
    "    <li>default number of neurons is 32, and the default number of inputs is 32 as well;</li>\n",
    "    <li> Unlike in previous tf example here weight tensor shape is 2D and it can be initialized to any matrix size on a call to object instantiation by setting number of neurons and inputs into the object instantiation.</li>\n",
    "    <li>Same with bias ‚Äì as many as units (neurons) that are initialized with 0‚Äôs<li>\n",
    "    <li>Initialize procedure sets inital weights w_init to be samples from random normal.</li>\n",
    "</ul>\n",
    "the output does the same forward computation as before with TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.__w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.__b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return tf.matmul(inputs, self.__w) + self.__b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can compute with this layer as before. So far not much of a difference with TF except we inherited all procedure of layer in KS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assumed that weights are matrices but they could be of any shape. We could use the tf procedure of non-declaring tape and then initalizing on the first call. However KS offers better method. We can only speccify number of units in the layer and then ise 'build' method to add input shape and weights. Remeber shapes are [ ] for const, [k] for vector size k, [m,n] for matrix, etc. Using add weights with standard initializer allows to initialize weights. In the example I assume that input is a matrix so I initialize by a  matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear1(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear1, self).__init__()\n",
    "        self.__units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.__w = self.add_weight(\n",
    "            shape=(input_shape[-1],self.__units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.__b = self.add_weight(\n",
    "            shape=(self.__units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        z=tf.matmul(x,self.__w) + self.__b\n",
    "        return tf.matmul(x,self.__w) + self.__b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the dimesions (i.e. call build) we just call the the class. However notice that build methood is invoked automatically on class so my program requires access to build to pass arguments, so I cannot have call __call__ hidden (so it has no underscores, same with build!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[2.0,1.0],[1.0,2.0]])\n",
    "#y_t=[[1.0],[1.0]]\n",
    "linear_layer = Linear1(4)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in TF Layers are composed into a model.\n",
    "## What needs to be done to define Keras Model\n",
    "<ul>\n",
    "    <li>We need to define how to put output of one layer into another</li>\n",
    "    <li>Model class is used to define the object we train. So there must be gradient descent method that we need to define</li>\n",
    "    <li>The Model class has the same interface as Layer, with the following differences:\n",
    "        <ul> \n",
    "            <li>It has built-in training, evaluation, and prediction loops model.fit(), model.evaluate(), model.predict().</li>\n",
    "            <li>It must be defined by the list of its inner layers, by the model.layers method.</li>\n",
    "            <li>It allows for saving and serialization using save(), save_weights(), etc. methods</li>\n",
    "        </ul>\n",
    "    </li>\n",
    " </ul>\n",
    " \n",
    "### How we do it\n",
    "\n",
    "<ul>\n",
    "    <li>There is a wide library of neurons that can be used for layer definition, no need to define your own. Once layers are defined they are composed into a model directly in Keras as long as layer output is defined. There are two ways to define a model:\n",
    " <ul>\n",
    "    <li> Sequential class. It is for stacks of layers, output of lower layer on stack is input of he next layer. This is the most common network architecture by far. It is done suing declaration\n",
    "        <ul>\n",
    "            <li>model = models.Sequential()</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Functional class. It allows for directed acyclic graphs of layers, which lets you build completely arbitrary architectures. It is done using declaration\n",
    "        <ul>\n",
    "            <li>model = models.Model(inputs=input_tensor, \t\toutputs=output_tensor)</li>\n",
    "        </ul>  \n",
    "        along with the declaration of how layers are connected.</li>\n",
    "    </ul>\n",
    "        We would get back to functional API many classes later</li>\n",
    "    <li> There is no need to define gradient descent for each layer in the model as long as one of the standard neurons is used in each layer of the model.</li> \n",
    "    <li>There is no need to specify loss functions in layer specification as long as only standard neurons are used</li> \n",
    "    <li>The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training.</li>\n",
    "<ul>\n",
    "Of course in the very beginning we need to import from keras models and layers classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the network consists 3 layers (beyond input):\n",
    "<ol>\n",
    "<li>first layer consist of 16 ReLU units that each take matrices with 10000 input features being one dimensios\n",
    "<li> Second takes input from first and contains 16 ReLU unints\n",
    "<li>And last is single neuron with sigmoid activation\n",
    "<li>The loss is binary cross-entropy, metric that we are maintaining is accuracy\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realistic example: imdb classification\n",
    "IMDB dataset is built into KS: \n",
    "<ul>\n",
    "    <li>a set of 50,000 highly polarized reviews from the Internet Movie Database.</li>\n",
    "    <li> Reviews have already been preprocessed: the sequences of words have been turned into sequences of integers, where each integer stands for a specific word in a dictionary</li>\n",
    "    <li>Each record has target variable ‚Äì class. The values are positive +1 and negative 0</li>\n",
    "    <li> Dataset is prepared: data is separated into set of training record and st of testing records. Each set consists of 25,000 reviews, of which 50% are negative reviews and 50% are positive reviews.\n",
    "    <li>Dataset contains wordindex that is a a dictionary that maps integers to words\n",
    "</ul>\n",
    "Let's see how does the original review looks like. Note that numeric code in the review encoding and in the dictionary differ by 3 (which is why i-3 is there). This is done to have values 0,1,2 in the encoding for service purposes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "print(train_data[0],'\\n')\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '\\n') for i in train_data[0]])\n",
    "print(decoded_review)\n",
    "print('the label of this review is: ', train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning process\n",
    "Beyond creating the model learning process includes\n",
    "<ul>\n",
    "    <li>Encoding the data that we need to learn with (done for imdb)</li> \n",
    "    <li>Separating data into training and testing subset (done for imbd)</li>\n",
    "    <li>Formatting data in the format arrpopriate for the model. This step somewhat depends on the model (needed)</li>\n",
    "    <li>Separating training data into training and validation data (needed) </li>\n",
    "    <li>Passing formatted input data (and the corresponding target data) to the model via the model.fit() method (needed)</li>\n",
    "    <li>Evaluating results of training on testing data using model.evaluate() (needed)</li>\n",
    "    <li>Predicting results using predict() method (needed)</li>\n",
    "</ul>\n",
    "Begin with creating model by importing necessary layers. But first import libraries needed for that. \n",
    "\n",
    "Feedforward fully connected network with a few layers of ReLU ($y=\\max‚Å°{(0,\\vec{ùë§}^ùëá\\cdot \\vec{ùë•}+ùëè)}$ units with logistic regression output layer performs well on problems when no specific structure exists like on sentences. How many layers and how many units per layer should we choose?\n",
    "<ul>\n",
    "    <li>First layer reduced input dimension vector to the output dimension that has dimensionality of number of units in a  layer</li>\n",
    "    <li>Every hidden layer can reduce or not reduce the dimension</li>\n",
    "</ul>\n",
    "So as one of the gurus (Bengio) of NN said ‚Äúunderstand dimensionality of your representation space as how much freedom you‚Äôre allowing the network to have when learning internal representations.‚Äù\n",
    "<ul>\n",
    "    <li>Having more hidden units (a higher-dimensional representation space) per layer allows your network to learn more-complex representations, but it makes the network more computationally expensive and may lead to learning unwanted patterns</li>\n",
    "    <li>Having more layers allows you learn more sophisticated patterns, but as with number of units you my learn unwanted non-characteristic patterns</li>\n",
    "</ul>\n",
    "Unfortunately there is no exact science on it ‚Äì the only way to establish it is by experimentation\n",
    "Rule of thumb:\n",
    "<ol>\n",
    "    <li>reduce the feature space in the first step to between 10 and 100</li>\n",
    "    <li>Start with relatively small number of hidden layers</li>\n",
    "</ol>\n",
    "We choose RMSprop that is a modification of stochastic gradient descent (gradientTape) that works well on FF (fully connected) networks. We need to do classification so it is sigmoid (logistic regression at the end. Binary cross-entropy is loss associated with output of logistic regression. We are going to watch accuracy of the model to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "imdb_model = models.Sequential()\n",
    "imdb_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "imdb_model.add(layers.Dense(16, activation='relu'))\n",
    "imdb_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "imdb_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create the function that converts sequence into vector of length 10,000. Thus all reviews together are going to form a matrix of dimension number of reviews x 10,000. Why 10,000? because the dictionary contain 10,000 words. So we assign a pair (review number,word number) value 0 if the word is not in the review and we assign to the pair 1 if the word is in the review. We initialize arry to all 0's and then in the loop we fill in each entry i,j by occurenc/non-occurence of j-th word in i'th review. Thus we obtain sparse matrix for reviews-by-words that is called results. Each row in results array is a vector of occurences of 10,000 wors in the corresponding review.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we vectorize taining and testing data and we also convert all labels into numpy array of real values. They are still 0 and 1 but it is going to be matching the input tensor type of output of the sigmoid layer that we had in the model which is madatory for using standard loss! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)\n",
    "print(x_train[0])\n",
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we separate training data into training and validation. Why have validation data set? It provides an unbiased evaluation of a model fit on the training data set while tuning the model's hyperparameters (e.g. the number of hidden units‚Äîlayers and layer widths‚Äîin a neural network). Testing data is used only once and are representative of the general population while validating data is part of training data, so it may be used many times in the process of desgin to fine-tune the model. Here we take first 10,000 records for trainin and the rest for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit the network to data. We take batch size 512 reviews and do 20 epochs training to start with. If loss stabilizes at the end and accuracy doesn't increase then we had enough epochs for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = imdb_model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data from training is preserved in results attribute '.history'. We watched accuracy and computed loss. So they must be there. Let's see what is in fact there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot these results. We need new library matplotlib.pyplot for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now plot these values as function of epoch number. Let's plot validation against training. First let's plot loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "plt.plot(epochs, acc, 'ro', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'g^', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after 3rd epoch validation stays flat and even decreses while test accuracy increases. We are overfitting the model. It maybe that data isn't representative or our model is too simple and doesn't capture the distribution. Needs to be adjusted. But it is not a production so we stop here assuming we done our best.\n",
    "\n",
    "Let's see how it performs on testing data using .evaluate() and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,acc = imdb_model.evaluate(x_test, y_test)\n",
    "print('mean probability of correct classification (1-loss) is : ',l,'\\n','accuracy is: ',acc)\n",
    "print(imdb_model.predict(x_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my_tf]",
   "language": "python",
   "name": "conda-env-my_tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
