\documentclass[12pt,english]{article}
\usepackage[a4paper,bindingoffset=0.2in,%
            left=1in,right=1in,top=1in,bottom=1in,%
            footskip=.25in]{geometry}
\usepackage{blindtext}
\usepackage{titling}
\usepackage{amssymb}
\usepackage{listofitems,amsmath}
\usepackage{listings}
\usepackage{lettrine} 
\usepackage{tikz}  
\usepackage{color} 
\usepackage{xcolor}
\usetikzlibrary{shapes, arrows, calc, arrows.meta, fit, positioning} % these are the parameters passed to the library to create the node graphs  
\tikzset{>=latex} % for LaTeX arrow head
\tikzset{  
    -Latex,auto,node distance =0.6 cm and 1.3 cm, thick,% node distance is the distance between one node to other, where 1.5cm is the length of the edge between the nodes  
    state/.style ={ellipse, draw, minimum width = 0.9 cm}, % the minimum width is the width of the ellipse, which is the size of the shape of vertex in the node graph  
    point/.style = {circle, draw, inner sep=0.18cm, fill, node contents={}},  
    bidirected/.style={Latex-Latex,dashed}, % it is the edge having two directions  
    el/.style = {inner sep=2.5pt, align=right, sloped}  
}  
\colorlet{mydarkblue}{blue!40!black}
\tikzstyle{connect}=[thick,mydarkblue] %,line cap=round
\tikzstyle{connect arrow}=[-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten <=0.5,shorten >=1]
\usepackage[outline]{contour} % glow around text
\setlength{\parskip}{12pt}
\title{Home Work 6 Undergraduate}
\date{\today}
\author{Jose Carlos Munoz}
%================================
\begin{document}
\newgeometry{left=0.8in,right=0.8in,top=1in,bottom=1in}
\begin{center}
    \Large
    \textbf{Homework 8}\\
    \small
    \today\\
    \large
    Jose Carlos Munoz
\end{center}
%===============================
\section*{2-8}
When the value of the entries are 0 or 1, we have two poissible outcomes. The loss for each entry with a value of 1 is $L_i-\log{(\phi ((M)_{ij})}$. When the entry value is 0, the loss will be $L_i-\log{( 1- \phi ((M)_{ij})}$. Here $\phi ()$ is the sigmoid function\\
When the Entries are a value between 0 and 1 we have a combination of the two losses at 0 and 1. This means that the entry that the function of the loss for this values will be $L_i = -y_i \log{(\phi ((M)_{ij}) -(1-y_i)\log{( 1- \phi ((M)_{ij})}}$. Where $y_i$ is the entry value.
\section*{4-6}
 Since the training data and the testing data are close in accuracy, we can conclude that the model is underfitting. To fix this, it would be good if we add more units that have non linearity
\section*{4-10}
The classification accuracy on the traiing data get worse as we increase the training data size. This is because the model can not remember the training data with limited capacity.\\
For the average loss, it increases  as we have more dta because the model is less specific for a particular training instance.\\
The testing data accuracy typically increases as we have more training data. So, at some point there will be a moment in which the training data and the testing data will be around the same accuracy.
\end{document}
