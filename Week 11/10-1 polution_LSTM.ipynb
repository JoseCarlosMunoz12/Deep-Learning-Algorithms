{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwODv2Y9rC_b"
   },
   "source": [
    "# Polution Time-Series Forecasting with LSTM\n",
    "The supervised pollution learning problem: predict the pollution at the current time $ùë°$ given the pollution measurement and weather conditions at the prior time steps $(ùë°‚àí1), \\ldots, (t-i)$. WE are going to model here the simplest version when $i=1$, i.e. predict from previous time step.<br> \n",
    "As usual we start with loading necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6943,
     "status": "ok",
     "timestamp": 1647030655502,
     "user": {
      "displayName": "Alexander Wolpert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj5BlqIcQzhGvVcFL2-YGYmoDjbdBVCj8S2EhfpAHM=s64",
      "userId": "05807256001004993747"
     },
     "user_tz": 360
    },
    "id": "KlaAnETdrC_e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T5Un-mdrC_g"
   },
   "source": [
    "## Polution Data\n",
    "The data set is taken from UCI data set repository: https://archive.ics.uci.edu/ml/machine-learning-databases/00381/  Click on data folder to download. I renamed it raw_pollution.csv.<br><br>\n",
    "Weather conditions are given by combination of feature values measured at a given date/time. We assume that we do not know all important features, so there is some uncertainty involved and it is encoded in the current unknow state of the weather system defined by undetermined  weather  conditions. <br>\n",
    "The data in the file includes the date-time, the pollution level called PM2.5 concentration, and the weather information. Table columns are:\n",
    "<ul>\n",
    "    <li>index column</li>\n",
    "    <li> year: year of data in this column</li>\n",
    "    <li>month: month of data in this colum</li>\n",
    "    <li>day: day of data in this column</li>\n",
    "    <li>hour: hour of data in this column</li>\n",
    "    <li>pm2.5: PM2.5 concentration column. This is the target column</li>\n",
    "    <li>DEWP: Dew Point column</li>\n",
    "    <li>TEMP: temperature column</li>\n",
    "    <li>PRES: pressure column</li>\n",
    "    <li>cbwd: Combined wind direction column</li>\n",
    "    <li>Iws: Cumulated wind speed column</li>\n",
    "    <li>Is: Cumulated hours of snow column</li>\n",
    "    <li>Ir: Cumulated hours of rain</li>\n",
    "</ul>\n",
    "When rpocessing we would need to convert \"year\",\"month\",\"day\", \"hour\" columns into $daytime$ Python object. For that I define parse function that is nothing more than a call to <b>strptime()</b> standard Python function. It takes takes two arguments:\n",
    "<ul>\n",
    "    <li>string (that be converted to datetime)</li>\n",
    "    <li>format code</li>\n",
    "</ul>\n",
    "Based on the string and format code used, the method returns its equivalent datetime object. For example, if the call is\n",
    "\n",
    "    - datetime.strptime(x, '%Y %m %d %H')\n",
    " then the format of argiment $x$ that is fed to the call must be \"year\",\"month\",\"day\", \"hour\", as for example in \"2010 1 5 17\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1647030662948,
     "user": {
      "displayName": "Alexander Wolpert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj5BlqIcQzhGvVcFL2-YGYmoDjbdBVCj8S2EhfpAHM=s64",
      "userId": "05807256001004993747"
     },
     "user_tz": 360
    },
    "id": "6SccxFhwrC_g"
   },
   "outputs": [],
   "source": [
    "def parse(x):\n",
    "    return datetime.strptime(x, '%Y %m %d %H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wROI9HLCrC_h"
   },
   "source": [
    "## Input data\n",
    "As usual check if we are in Google colab or working locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1647030665260,
     "user": {
      "displayName": "Alexander Wolpert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj5BlqIcQzhGvVcFL2-YGYmoDjbdBVCj8S2EhfpAHM=s64",
      "userId": "05807256001004993747"
     },
     "user_tz": 360
    },
    "id": "Ih1xbVlTrC_i"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB=True\n",
    "except:\n",
    "    IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgFeseBMrC_j"
   },
   "source": [
    "Set the path to the directory into which the original dataset is located. New Python twist: I assume that data is located in a subdirectory of a directory from which we launched the program. So I take its name using \"os.getcwd\".<br>\n",
    "The subdirectory in which I assume the data is located is ../Data/pollution when working locally and in ../My Drive/courses/Deep Learning/data/pollution when on Colab. If there is no data file in this subdirectory, then I exit.<br>\n",
    "My original file is named \"raw_pollution.csv\". But I do not want to wreck an original file as I'll be preprocessing data, so I'll preprocess it into a new file \"pollution.csv\" in the next cell if I have not done it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 761,
     "status": "ok",
     "timestamp": 1647031150848,
     "user": {
      "displayName": "Alexander Wolpert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj5BlqIcQzhGvVcFL2-YGYmoDjbdBVCj8S2EhfpAHM=s64",
      "userId": "05807256001004993747"
     },
     "user_tz": 360
    },
    "id": "4gxfgXHzrC_j",
    "outputId": "c5d71edc-efe1-4051-c2f0-88f4b82505bc"
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "#    dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "    dir_path = os.getcwd()\n",
    "    data_dir = os.path.join(dir_path, 'pollution')\n",
    "else:\n",
    "    drive.mount('/content/gdrive')\n",
    "    dir_path = os.path.dirname(os.path.realpath(os.path.abspath('')))\n",
    "    data_dir = os.path.join(dir_path, '/content/gdrive/My Drive/courses/Deep Learning/data/pollution')\n",
    "if not os.path.exists(data_dir):\n",
    "    exit(3)\n",
    "copy=False\n",
    "dst=os.path.join(data_dir,'pollution.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, if i have not created processed file \"pollution.csv\" before I read the original file \"raw_pollution.csv\" and process it and write out processed file  \"pollution.csv\". If I already processed raw file then I just read it.<br>\n",
    "How do I process raw file if I need it? I extensively use \"pandas\" module: \n",
    "<ol>\n",
    "    <li> I read the raw data file into \"dataset\" data-frame variable. I use \"date_parser\" function of pandas \"read_csv\" module to create a column with datetime object that replaces 4 columns - \"year\",\"month\",\"day\", \"hour\". Data_parser converts a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion but I use my own parse function. Read_csv passes value of \"parse_dates\" argument concatenating (row-wise) the string values from the columns defined by parse_dates into a single array and pass that to the parser. The resulting column becomes index column of the dataframe</li>\n",
    "    <li> I use pandas dataframe.drop to drop index column (name is 'No', axis=1 means columns, inplaceTrue means do not make a copy - just do it on the frame)</li>\n",
    "    <li> I define new names of the columns of dataframe  replacing \"pm2.5  DEWP  TEMP    PRES cbwd    Iws  Is  Ir\" with  \"pollution  dew  temp   press wnd_dir  wnd_spd  snow  rain\" note that name replacement can be applied to a list of all coumns except for index column which can only be renamed explicitly.</li>\n",
    "    <li> which is what is done next - index column is named \"date\">/li>\n",
    "     <li> I replace 'NA' in pollution column with 0 and drop first 24 hours of observations as you can see (look into raw file) it has no pollution data there</li>\n",
    "    <li>save dataset using pandas to_csv method.</li>\n",
    "</ol>\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(x):\n",
    "    return datetime.strptime(x, '%Y %m %d %H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dst):\n",
    "    copy=True\n",
    "    src = os.path.join(data_dir,'raw_pollution.csv')\n",
    "    dataset = read_csv(src,  parse_dates = [['year', 'month', 'day', 'hour']], index_col=0, date_parser=parse)\n",
    "    dataset.drop('No', axis=1, inplace=True)\n",
    "    dataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n",
    "    dataset.index.name = 'date'\n",
    "    dataset['pollution'].fillna(0, inplace=True)\n",
    "    dataset = dataset[24:]\n",
    "    dataset.to_csv(dst)\n",
    "    print(dataset[:5])\n",
    "if not copy:\n",
    "    dataset = read_csv(dst, header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then plot all columns. To do so I need to conver a dataframe to a 2:d array, i.e. get rid of all data frame things such as column names, axises, index column designations, etc. This is what datafra.values method does. And then plotting of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dataset.values\n",
    "# specify columns to plot\n",
    "groups=list(range(0,7+1))\n",
    "i = 1\n",
    "# plot each column\n",
    "plt.figure()\n",
    "for group in groups:\n",
    "    plt.subplot(len(groups), 1, i)\n",
    "    plt.plot(values[:, group])\n",
    "    plt.title(dataset.columns[group], y=0.5, loc='right')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize, reformat and prepare data for learning\n",
    "<ul>\n",
    "    <li> Import from sklearn label encoder and scaler</li>\n",
    "    <li> Using sklearn.preprocessing.LabelEncoder.fit_transform method take categorical values of the column 4 (wind) and convert these values to integers by enumerating the categorical values of the column </li>\n",
    "    <li> Convert all integers to real values for subsequent scaling</li>\n",
    "    <li>Using sklearn.preprocessing.MinMaxScaler.fit_transform method take all columns and uniformly scale values in columns between 0 and 1</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### series_to_supervised() Function\n",
    "Next we convert data to format that is required to use LSTM models.¬†This is done using series_to_supervised function borrowed from https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/ (this is not my function, but it is free to use as allowed by the author). <br>\n",
    "Technically, when we are forecusting time series (or any sequence) at the current time (position) $ùë°$ for the future times (positions) $(ùë°+1, \\ldots, ùë°+ùëõ)$ we are changing time origin: from the forecast times in future and to predicting from the past observations, i.e. we are taking as current the time as the time in which we want the prediction, so $t^{'}=t+n$, and we are computing it from backward values, i.e. we are computing value at $t^{'}$ from values $(ùë°^{'}‚àí1, ùë°^{'}‚àíùëõ)$ that are used to make forecasts. Thus to make this view change we (i.e. time series data into a format on which we can apply supervised learning) we use the Pandas¬†shift()¬†function. The DataFrame.shift()¬†create copies of columns pushed forward (rows of $NaN$ values added to the front) or pulled back (rows of $NaN$ values added to the end) a number of time units that are needed for prediction. This number is the argument of DataFrame.shift(). This function can be used to create new columns (features) of lag observations and the column of forecast observations for target variable necessary for supervised learning. Here is mock example. Suppose we need to predict a series of a single variable that only deneds on its own previous $2$ values and we wnat to predict next $2$ values for a currnt time. Suppose we have the following $9$ observations. Then for our prediction we'll need to have $14 \\times 5$ dataframe on which we'll be learning. Then we'll need to drop all rows that conatin \"na\". So the transformation is <br>\n",
    "<center>\n",
    "$\\begin{array}{l} 2\\\\ 3\\\\ 4\\\\ 3\\\\ 4\\\\ 5\\\\ 4 \\\\ 4\\\\ 5 \\end{array}\\ \\ \\ $ becomes $\\ \\ \\ \\begin{array}{|l|l|l|l|l|} 2 & na & na &na & na \\\\ \\hline 3 & 2 & na & na & na\\\\ \\hline 4 & 3 & 2 & na & na \\\\ \\hline 3 & 4 & 3 & 3 & na\\\\ \\hline 4 & 3 & 4 & 3 & 2 \\\\ \\hline 5 & 4 & 3 & 4 & 3\\\\ \\hline 4 & 5 & 4 & 3 & 4\\\\ \\hline 4 & 4 & 5 & 4 & 4 \\\\ \\hline 5 & 4 & 4 & 5 & 4\\\\ \\hline na & 5 & 4 & 4 & 5\\\\ \\hline na & na &5 & 4 & 4 \\\\ \\hline na & na & na & 5 & 4 \\\\ \\hline na & na & na & na & 5 \\end{array}\\ \\ \\ $ becomes $\\ \\ \\ \\begin{array}{|l|l|l|l|l|} 4 & 3 & 4 & 3 & 2 \\\\ \\hline 5 & 4 & 3 & 4 & 3\\\\ \\hline 4 & 5 & 4 & 3 & 4\\\\ \\hline 4 & 4 & 5 & 4 & 4 \\\\ \\hline 5 & 4 & 4 & 5 & 4 \\end{array}\\ \\ \\ $</center>\n",
    "<br> \n",
    "Arguments of series_to_supervised are:\n",
    "<ul>\n",
    "    <li> data: Sequence of observations as a list or 2D NumPy array.</li>\n",
    "    <li>n_in: Number of lag observations as input (ùëã). Values may be between [1..len(data)].</li>\n",
    "    <li> n_out: Number of observations as output (ùëå). Values may be between [0..len(data)-1].</li>\n",
    "    <li> dropnan: Boolean whether or not to drop rows with NaN values.</li>\n",
    "</ul>\n",
    "The function returns It returns Pandas DataFrame of series framed for supervised learning: if we had $ùëõ\\times ùëù$ data frame with ùëù  features and ùëõ time slices then if we want to have new data dependent on $ùëò$ previous time values $ùë°‚àí1,\\ldots, ùë°‚àíùëò$ and we are predicting $ùë†$ new time values at times $ùë°,\\ldots,ùë°+ùë†‚àí1$ then the new data frame has dimensions $(ùëõ\\times p)\\cdot (ùëò+ùë†+1)$. \n",
    "<br>\n",
    "Essentially this function\n",
    "<ol>\n",
    "    <li>Shifts columns forward n_in times to create $ùë°‚àíùëñ$ columns and n-out times backward from $t$ to create $ùë°+ùëñ$ feature vectors</li>\n",
    "    <li>Each time a new frame of feature vectors shifted by time $ùë°\\pm ùëñ$ a set of $ùëù$ columns is created. It is treated as a set of additional features i.e. it is merged with the previous data frame adding to it $ùëù$ new features</li>\n",
    "    <li>It starts with checking if it is one dimensional or multidimensional variable prediction. In both cases define n_var number of dimensions/variables (either 1 or # of columns)</li>\n",
    " <li> If it is multidimensional then convert numpy.ndarray to pandas DataFrame and create empty lists where we‚Äôll put in data</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1] # #-of-vars is either 1 if one dim sequence or number of cols\n",
    "    df = DataFrame(data) #convert from array of matrix to data frame\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))       #shift all frame columns by i (1 )up obtaining t-i (t-1) values\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)] #form names of vars from 1 to 8 at time t-i\n",
    "        # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))   # put frame back in place\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "        return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am only doing $t+1$ prediction we need number of lag observations for input to be 1 and number of observations to output be 1. Drop columns in ùë°+1 that are not target variables (not pollution)from added frames. Pandas dataframe.drop method allows to dropSinc we reproduced/concatenated all columns (axis is 1 so it is indeed columns), and we do not need for pprediction all feature columns at time $t+1$ so we earse these columns. Note that columns at time $t+1$ aren't part of the model only feature columns of t-1 are. But if we were predicting the dependence on 2 time slots back then features of $t+1$ would be needed. Moreover I treat previous polution value as a feature on which next polluiton value depends. But if we were not using it as a feature we would have had to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data\n",
    "By using dataframe.values method only the values in the DataFrame will be returned, the axes labels will be removed. Could use to_numpy instead. Then I split index number into 3 groups: train, validation and test sets of index numbers. Using each index group respectively create training data, validation data and testing data. Split data for each group into independent feature variables and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = reframed.values\n",
    "n_train_hours = int(round(len(values)/3))\n",
    "n_valid_hours=int(round(2*len(values)/3))\n",
    "train = values[:n_train_hours, :]\n",
    "valid=values[n_train_hours:n_valid_hours, :]\n",
    "test = values[n_valid_hours:, :]\n",
    "# split into training input, validation output and later testing input data\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "valid_X, valid_y = valid[:, :-1], valid[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "valid_X = valid_X.reshape((valid_X.shape[0], 1, valid_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, valid_X.shape, valid_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and fit the model\n",
    "A do standard vanilla LSTM is an LSTM model that has a single hidden layer of LSTM units, and an output layer used to make a prediction. I define the LSTM layer with 50 LSTM neurons in the only hidden layer and 1 dense/linear neuron in the output layer for predicting pollution. Multiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model. It is necessary when we want to discover complex time dependencies just like in convolutional nets. We do not need it here. Why 50 neurons? it is a practical matter because it is bit enough number to carry memmory of previous steps and small enough to not have vanishing gradiaent problem. \n",
    "<ul>\n",
    "    <li>The input shape has 1 time step with 8 features</li>.\n",
    "    <li>Optimizer is Adam version of stochastic gradient descent. Works well for LSTMs - later if wqe have time</li>\n",
    "    <li> Loss is Mean Absolute Error (MAE)</li>\n",
    "    <li>Batch size 96 (hours = 4 days)</li>\n",
    "</ul>\n",
    "Why did I choose MAE loss?The beauty of the MAE is that since we are taking the absolute value, all of the errors will be weighted on the same linear scale. Thus, we won‚Äôt be putting too much weight on our outliers as MSE function does. This loss evens out errors of how well our model is performing which is good for a volatile target such as pollution.\n",
    "<br>\n",
    "The standard measures of quality of the model for time series are:\n",
    "<ul>\n",
    "    <li> Mean absolute error $\\frac{1}{n}\\sum_{i=1}^n|y_i -\\hat{y}_i|$ </li>\n",
    "    <li> Mean squared error (MSE) $\\frac{1}{n}\\sum_{i=1}^n(y_i -\\hat{y}_i)^2$</li>\n",
    "    <li> Root MSE (RMSE) $\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(y_i -\\hat{y}_i)^2}$</li>\n",
    "</ul>\n",
    "I choose to watch mean squared error exactly to see how much my model evens out. Am I missing that my model is underfiting which I didn't noitce in losse because of evening all errors out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mse'])\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=96, validation_data=(valid_X, valid_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.plot(history.history['mse'], 'r',label='MSE')\n",
    "plt.plot(history.history['val_mse'], 'b', label='Validation MSE')\n",
    "plt.title('Training and validation Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='valid')\n",
    "plt.title('Training and validation Loss (MAE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import concatenate\n",
    "\n",
    "yhat = model.predict(test_X)\n",
    "rmse0 = sqrt(mean_squared_error(test_y, yhat))\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE scaled: %.3f' % rmse0)\n",
    "print('Test RMSE absolute: %.3f' % rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional reading\n",
    "Pollution modeling: https://luisdamiano.github.io/work/gsoc17_iohmm_financial_time_series.html#:~:text=Code%202017%20program.-,1%20The%20Input%2DOutput%20Hidden%20Markov%20Model,control%20signal%2C%20to%20output%20sequences\n",
    "https://towardsdatascience.com/forecasting-air-pollution-with-recurrent-neural-networks-ffb095763a5c\n",
    "Chollet, https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.3-advanced-usage-of-recurrent-neural-networks.ipynb "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "10-1 polution_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:my_tf]",
   "language": "python",
   "name": "conda-env-my_tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
